# Sloposcope Documentation

## Overview

Sloposcope is an AI slop detection system that identifies low-quality, verbose, or stylistically padded text generated by Large Language Models (LLMs). It implements a research-based 7-dimensional framework for comprehensive text quality analysis.

## Core Concepts

### What is AI Slop?

AI slop refers to low-quality text generated by LLMs that exhibits:

- Excessive verbosity without substance
- Repetitive or templated patterns
- Inappropriate tone or jargon
- Poor coherence and flow
- Lack of relevance to context
- Factual inaccuracies
- Biased or subjective language

### The 7 Core Dimensions

Based on "Measuring AI 'SLOP' in Text" (Shaib et al., 2025):

1. **Density** - Information content per word
2. **Repetition** - N-gram repetition and compression
3. **Templated** - Formulaic and boilerplate patterns
4. **Tone** - Jargon and awkward phrasing
5. **Coherence** - Entity continuity and topic flow
6. **Relevance** - Appropriateness to context
7. **Factuality** - Accuracy and truthfulness

## Usage Examples

### CLI Analysis

```bash
# Basic analysis
python -m sloplint.cli analyze "This is a test sentence."

# Detailed analysis with explanations
python -m sloplint.cli analyze document.txt --explain --spans

# Domain-specific analysis
python -m sloplint.cli analyze article.txt --domain news --explain
```

### Web Interface

1. Start the server: `uvicorn app:app --reload`
2. Open http://localhost:8000
3. Paste your text and click "Analyze Text"
4. View detailed results with visual indicators

### API Integration

```python
import requests

response = requests.post("http://localhost:8000/analyze", json={
    "text": "Your text here",
    "domain": "general",
    "explain": True
})

result = response.json()
print(f"Slop Score: {result['slop_score']:.3f} ({result['level']})")
```

## Understanding Results

### Slop Score (0.0 - 1.0)

- **0.0-0.50**: Clean (high-quality text)
- **0.50-0.70**: Watch (some AI patterns)
- **0.70-0.85**: Sloppy (clear AI characteristics)
- **0.85-1.0**: High-Slop (obvious AI generation)

### Confidence (0.0 - 1.0)

- **1.0**: High confidence in analysis
- **0.5-0.9**: Moderate confidence
- **<0.5**: Low confidence (may need more text)

### Individual Metrics

Each dimension provides specific metrics:

- **Density**: Perplexity, idea density, semantic density
- **Repetition**: N-gram repetition, compression ratio
- **Templated**: Boilerplate hits, pattern repetition
- **Tone**: Hedging, sycophancy, formality ratios
- **Coherence**: Entity continuity, embedding drift
- **Relevance**: Similarity scores, relevance variance
- **Factuality**: Unsupported claims, contradictions

## Configuration

### Domains

- **general**: Default, balanced analysis
- **news**: Optimized for journalistic content
- **qa**: Focused on question-answer content

### Customization

The system can be customized through:

- Domain-specific weights
- Threshold adjustments
- Feature selection
- Calibration data

## Troubleshooting

### Common Issues

1. **Low confidence scores**: Try longer text samples
2. **Inconsistent results**: Check domain selection
3. **Performance issues**: Ensure sufficient memory
4. **Model errors**: Verify spaCy model installation

### Getting Help

- Check the GitHub issues
- Review the API documentation at `/docs`
- Test with the provided sample texts

## Development

### Architecture

```
sloplint/
├── cli.py              # Command-line interface
├── feature_extractor.py # Main analysis orchestrator
├── combine.py          # Score normalization and combination
├── features/           # Individual dimension analyzers
├── nlp/               # NLP pipeline and processing
└── spans.py           # Character-level span detection
```

### Adding New Features

1. Create feature extractor in `features/`
2. Add dimension mapping in `combine.py`
3. Update CLI and web interface
4. Add tests and documentation

### Testing

```bash
# Run all tests
make test

# Run specific test categories
make test-unit
make test-coverage
```

## Research Background

This implementation is based on academic research that identified the most effective patterns for detecting AI-generated text quality issues. The system prioritizes:

- **Precision over recall**: Minimize false positives
- **Interpretability**: Clear explanations for each metric
- **Scalability**: Efficient processing for production use
- **Flexibility**: Adaptable to different content types

## Performance Considerations

- **Memory**: ~400MB peak usage
- **Speed**: <1s for 1000 words
- **Accuracy**: Research-validated thresholds
- **Scalability**: Batch processing support

## Future Enhancements

- Machine learning integration
- Real-time span detection
- Multi-language support
- Advanced calibration options
- Performance optimizations
